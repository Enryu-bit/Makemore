# Makemore: Autoregressive Character-Level Language Model<br>
makemore is an autoregressive character-level language model designed to generate unique, plausible text sequences by learning the statistical patterns of an input dataset. Built from first principles to deconstruct the mechanics of sequence modeling, this repository implements a progression of architectures ranging from simple probabilistic Bigram models to Multi-Layer Perceptrons (MLP), Recurrent Neural Networks (RNN/GRU), and WaveNet. The system treats language generation as a probabilistic task, modeling the conditional probability distribution of the next character given a context window. It serves as a foundational study in how modern Large Language Models operate, moving from basic lookup tables to high-dimensional embeddings and backpropagation, ultimately "making more" of whatever data it is fed (e.g., generating infinite lists of novel names).